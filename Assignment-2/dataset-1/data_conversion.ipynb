{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = './Medical Llama Performance Dataset.xlsx'\n",
    "h5_path = './medical_llama_performance_dataset.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel(excel_path, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict,\n",
       " dict_keys(['Overview', 'Model Performance Data', 'Feature Description', 'Metrics Information', 'Data Lineage']))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(excel_data), excel_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data, metadata1, metadata2, provenance = excel_data.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### write to H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Name</th>\n",
       "      <th>Training type</th>\n",
       "      <th>Average Performance</th>\n",
       "      <th>Owner</th>\n",
       "      <th>License</th>\n",
       "      <th>Parameter Size</th>\n",
       "      <th>Context Window</th>\n",
       "      <th>Base LLAMA Model</th>\n",
       "      <th>LLAMA pre-trained flavor</th>\n",
       "      <th>...</th>\n",
       "      <th>Language</th>\n",
       "      <th>Med -MCQA</th>\n",
       "      <th>Med -QA</th>\n",
       "      <th>MMLU-Anatomy</th>\n",
       "      <th>MMLU-Clinical Knowledge</th>\n",
       "      <th>MMLU-Collage Biology</th>\n",
       "      <th>MMLU-College Medicin</th>\n",
       "      <th>MMLU-Medical Genetics</th>\n",
       "      <th>MMLU-Professional Medicina</th>\n",
       "      <th>PubMed-QA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://huggingface.co/ProbeMedicalYonseiMAILa...</td>\n",
       "      <td>ProbeMedicalYonseiMAILab/medllama3-v20</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>90.01</td>\n",
       "      <td>ProbeMedicalYonseiMAILab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>med-LLAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>75.40</td>\n",
       "      <td>81.07</td>\n",
       "      <td>91.85</td>\n",
       "      <td>95.85</td>\n",
       "      <td>98.61</td>\n",
       "      <td>94.80</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.90</td>\n",
       "      <td>75.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://huggingface.co/aaditya/OpenBioLLMLlama...</td>\n",
       "      <td>aaditya/OpenBioLLMLlama-70B</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>86.06</td>\n",
       "      <td>Saama AI Lab</td>\n",
       "      <td>llama3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>Meta-LLAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>74.01</td>\n",
       "      <td>78.16</td>\n",
       "      <td>83.90</td>\n",
       "      <td>92.93</td>\n",
       "      <td>93.83</td>\n",
       "      <td>85.75</td>\n",
       "      <td>93.2</td>\n",
       "      <td>93.75</td>\n",
       "      <td>78.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://huggingface.co/skumar9/Llama-medx_v3.2</td>\n",
       "      <td>skumar9/Llama-medx_v3.2</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>75.42</td>\n",
       "      <td>skumar9</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>LLamaMerge</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>60.53</td>\n",
       "      <td>61.04</td>\n",
       "      <td>77.04</td>\n",
       "      <td>82.26</td>\n",
       "      <td>86.81</td>\n",
       "      <td>72.83</td>\n",
       "      <td>84.0</td>\n",
       "      <td>81.25</td>\n",
       "      <td>73.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://huggingface.co/abhinand/Llama-3-OpenBi...</td>\n",
       "      <td>abhinand/Llama-3-OpenBioMed-8B-slerp-v0.3</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>73.56</td>\n",
       "      <td>abhinand</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>Llama slerp Merge</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>61.58</td>\n",
       "      <td>61.27</td>\n",
       "      <td>71.11</td>\n",
       "      <td>77.74</td>\n",
       "      <td>84.03</td>\n",
       "      <td>71.10</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.04</td>\n",
       "      <td>74.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://huggingface.co/lighteternal/Llama3-mer...</td>\n",
       "      <td>lighteternal/Llama3-merge-biomed-8b</td>\n",
       "      <td>instruction-tuned</td>\n",
       "      <td>73.55</td>\n",
       "      <td>lighteternal</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>LLamaMerge</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.44</td>\n",
       "      <td>62.61</td>\n",
       "      <td>72.59</td>\n",
       "      <td>77.74</td>\n",
       "      <td>82.64</td>\n",
       "      <td>68.79</td>\n",
       "      <td>84.0</td>\n",
       "      <td>77.94</td>\n",
       "      <td>73.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://huggingface.co/ProbeMedicalYonseiMAILa...   \n",
       "1  https://huggingface.co/aaditya/OpenBioLLMLlama...   \n",
       "2     https://huggingface.co/skumar9/Llama-medx_v3.2   \n",
       "3  https://huggingface.co/abhinand/Llama-3-OpenBi...   \n",
       "4  https://huggingface.co/lighteternal/Llama3-mer...   \n",
       "\n",
       "                                        Name      Training type  \\\n",
       "0     ProbeMedicalYonseiMAILab/medllama3-v20         fine-tuned   \n",
       "1                aaditya/OpenBioLLMLlama-70B         fine-tuned   \n",
       "2                    skumar9/Llama-medx_v3.2         fine-tuned   \n",
       "3  abhinand/Llama-3-OpenBioMed-8B-slerp-v0.3         fine-tuned   \n",
       "4        lighteternal/Llama3-merge-biomed-8b  instruction-tuned   \n",
       "\n",
       "   Average Performance                     Owner     License  Parameter Size  \\\n",
       "0                90.01  ProbeMedicalYonseiMAILab         NaN             8.0   \n",
       "1                86.06              Saama AI Lab      llama3            70.0   \n",
       "2                75.42                   skumar9  apache-2.0             8.0   \n",
       "3                73.56                  abhinand      llama3             8.0   \n",
       "4                73.55              lighteternal      llama3             8.0   \n",
       "\n",
       "   Context Window Base LLAMA Model LLAMA pre-trained flavor  ... Language  \\\n",
       "0             NaN           LLAMA3                med-LLAMA  ...       en   \n",
       "1             NaN           LLAMA3               Meta-LLAMA  ...       en   \n",
       "2             NaN           LLAMA3               LLamaMerge  ...       en   \n",
       "3           256.0           LLAMA3        Llama slerp Merge  ...       en   \n",
       "4             NaN           LLAMA3               LLamaMerge  ...      NaN   \n",
       "\n",
       "  Med -MCQA Med -QA MMLU-Anatomy  MMLU-Clinical Knowledge  \\\n",
       "0     75.40   81.07        91.85                    95.85   \n",
       "1     74.01   78.16        83.90                    92.93   \n",
       "2     60.53   61.04        77.04                    82.26   \n",
       "3     61.58   61.27        71.11                    77.74   \n",
       "4     62.44   62.61        72.59                    77.74   \n",
       "\n",
       "   MMLU-Collage Biology  MMLU-College Medicin  MMLU-Medical Genetics  \\\n",
       "0                 98.61                 94.80                   98.0   \n",
       "1                 93.83                 85.75                   93.2   \n",
       "2                 86.81                 72.83                   84.0   \n",
       "3                 84.03                 71.10                   82.0   \n",
       "4                 82.64                 68.79                   84.0   \n",
       "\n",
       "   MMLU-Professional Medicina  PubMed-QA  \n",
       "0                       98.90      75.60  \n",
       "1                       93.75      78.97  \n",
       "2                       81.25      73.00  \n",
       "3                       79.04      74.20  \n",
       "4                       77.94      73.20  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 23 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Link                        40 non-null     object \n",
      " 1   Name                        40 non-null     object \n",
      " 2   Training type               40 non-null     object \n",
      " 3   Average Performance         40 non-null     float64\n",
      " 4   Owner                       40 non-null     object \n",
      " 5   License                     39 non-null     object \n",
      " 6   Parameter Size              39 non-null     float64\n",
      " 7   Context Window              16 non-null     float64\n",
      " 8   Base LLAMA Model            40 non-null     object \n",
      " 9   LLAMA pre-trained flavor    35 non-null     object \n",
      " 10  Precision                   40 non-null     object \n",
      " 11  Quantization                25 non-null     object \n",
      " 12  Any Other Flavors           19 non-null     object \n",
      " 13  Language                    28 non-null     object \n",
      " 14  Med -MCQA                   40 non-null     float64\n",
      " 15  Med -QA                     40 non-null     float64\n",
      " 16  MMLU-Anatomy                40 non-null     float64\n",
      " 17  MMLU-Clinical Knowledge     40 non-null     float64\n",
      " 18  MMLU-Collage Biology        40 non-null     float64\n",
      " 19  MMLU-College Medicin        40 non-null     float64\n",
      " 20  MMLU-Medical Genetics       40 non-null     float64\n",
      " 21  MMLU-Professional Medicina  40 non-null     float64\n",
      " 22  PubMed-QA                   40 non-null     float64\n",
      "dtypes: float64(12), object(11)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_hdf(h5_path, key='data', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### read H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Name</th>\n",
       "      <th>Training type</th>\n",
       "      <th>Average Performance</th>\n",
       "      <th>Owner</th>\n",
       "      <th>License</th>\n",
       "      <th>Parameter Size</th>\n",
       "      <th>Context Window</th>\n",
       "      <th>Base LLAMA Model</th>\n",
       "      <th>LLAMA pre-trained flavor</th>\n",
       "      <th>...</th>\n",
       "      <th>Language</th>\n",
       "      <th>Med -MCQA</th>\n",
       "      <th>Med -QA</th>\n",
       "      <th>MMLU-Anatomy</th>\n",
       "      <th>MMLU-Clinical Knowledge</th>\n",
       "      <th>MMLU-Collage Biology</th>\n",
       "      <th>MMLU-College Medicin</th>\n",
       "      <th>MMLU-Medical Genetics</th>\n",
       "      <th>MMLU-Professional Medicina</th>\n",
       "      <th>PubMed-QA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://huggingface.co/ProbeMedicalYonseiMAILa...</td>\n",
       "      <td>ProbeMedicalYonseiMAILab/medllama3-v20</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>90.01</td>\n",
       "      <td>ProbeMedicalYonseiMAILab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>med-LLAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>75.40</td>\n",
       "      <td>81.07</td>\n",
       "      <td>91.85</td>\n",
       "      <td>95.85</td>\n",
       "      <td>98.61</td>\n",
       "      <td>94.80</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.90</td>\n",
       "      <td>75.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://huggingface.co/aaditya/OpenBioLLMLlama...</td>\n",
       "      <td>aaditya/OpenBioLLMLlama-70B</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>86.06</td>\n",
       "      <td>Saama AI Lab</td>\n",
       "      <td>llama3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>Meta-LLAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>74.01</td>\n",
       "      <td>78.16</td>\n",
       "      <td>83.90</td>\n",
       "      <td>92.93</td>\n",
       "      <td>93.83</td>\n",
       "      <td>85.75</td>\n",
       "      <td>93.2</td>\n",
       "      <td>93.75</td>\n",
       "      <td>78.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://huggingface.co/skumar9/Llama-medx_v3.2</td>\n",
       "      <td>skumar9/Llama-medx_v3.2</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>75.42</td>\n",
       "      <td>skumar9</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>LLamaMerge</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>60.53</td>\n",
       "      <td>61.04</td>\n",
       "      <td>77.04</td>\n",
       "      <td>82.26</td>\n",
       "      <td>86.81</td>\n",
       "      <td>72.83</td>\n",
       "      <td>84.0</td>\n",
       "      <td>81.25</td>\n",
       "      <td>73.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://huggingface.co/abhinand/Llama-3-OpenBi...</td>\n",
       "      <td>abhinand/Llama-3-OpenBioMed-8B-slerp-v0.3</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>73.56</td>\n",
       "      <td>abhinand</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>Llama slerp Merge</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>61.58</td>\n",
       "      <td>61.27</td>\n",
       "      <td>71.11</td>\n",
       "      <td>77.74</td>\n",
       "      <td>84.03</td>\n",
       "      <td>71.10</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.04</td>\n",
       "      <td>74.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://huggingface.co/lighteternal/Llama3-mer...</td>\n",
       "      <td>lighteternal/Llama3-merge-biomed-8b</td>\n",
       "      <td>instruction-tuned</td>\n",
       "      <td>73.55</td>\n",
       "      <td>lighteternal</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLAMA3</td>\n",
       "      <td>LLamaMerge</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.44</td>\n",
       "      <td>62.61</td>\n",
       "      <td>72.59</td>\n",
       "      <td>77.74</td>\n",
       "      <td>82.64</td>\n",
       "      <td>68.79</td>\n",
       "      <td>84.0</td>\n",
       "      <td>77.94</td>\n",
       "      <td>73.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://huggingface.co/ProbeMedicalYonseiMAILa...   \n",
       "1  https://huggingface.co/aaditya/OpenBioLLMLlama...   \n",
       "2     https://huggingface.co/skumar9/Llama-medx_v3.2   \n",
       "3  https://huggingface.co/abhinand/Llama-3-OpenBi...   \n",
       "4  https://huggingface.co/lighteternal/Llama3-mer...   \n",
       "\n",
       "                                        Name      Training type  \\\n",
       "0     ProbeMedicalYonseiMAILab/medllama3-v20         fine-tuned   \n",
       "1                aaditya/OpenBioLLMLlama-70B         fine-tuned   \n",
       "2                    skumar9/Llama-medx_v3.2         fine-tuned   \n",
       "3  abhinand/Llama-3-OpenBioMed-8B-slerp-v0.3         fine-tuned   \n",
       "4        lighteternal/Llama3-merge-biomed-8b  instruction-tuned   \n",
       "\n",
       "   Average Performance                     Owner     License  Parameter Size  \\\n",
       "0                90.01  ProbeMedicalYonseiMAILab         NaN             8.0   \n",
       "1                86.06              Saama AI Lab      llama3            70.0   \n",
       "2                75.42                   skumar9  apache-2.0             8.0   \n",
       "3                73.56                  abhinand      llama3             8.0   \n",
       "4                73.55              lighteternal      llama3             8.0   \n",
       "\n",
       "   Context Window Base LLAMA Model LLAMA pre-trained flavor  ... Language  \\\n",
       "0             NaN           LLAMA3                med-LLAMA  ...       en   \n",
       "1             NaN           LLAMA3               Meta-LLAMA  ...       en   \n",
       "2             NaN           LLAMA3               LLamaMerge  ...       en   \n",
       "3           256.0           LLAMA3        Llama slerp Merge  ...       en   \n",
       "4             NaN           LLAMA3               LLamaMerge  ...      NaN   \n",
       "\n",
       "  Med -MCQA Med -QA MMLU-Anatomy  MMLU-Clinical Knowledge  \\\n",
       "0     75.40   81.07        91.85                    95.85   \n",
       "1     74.01   78.16        83.90                    92.93   \n",
       "2     60.53   61.04        77.04                    82.26   \n",
       "3     61.58   61.27        71.11                    77.74   \n",
       "4     62.44   62.61        72.59                    77.74   \n",
       "\n",
       "   MMLU-Collage Biology  MMLU-College Medicin  MMLU-Medical Genetics  \\\n",
       "0                 98.61                 94.80                   98.0   \n",
       "1                 93.83                 85.75                   93.2   \n",
       "2                 86.81                 72.83                   84.0   \n",
       "3                 84.03                 71.10                   82.0   \n",
       "4                 82.64                 68.79                   84.0   \n",
       "\n",
       "   MMLU-Professional Medicina  PubMed-QA  \n",
       "0                       98.90      75.60  \n",
       "1                       93.75      78.97  \n",
       "2                       81.25      73.00  \n",
       "3                       79.04      74.20  \n",
       "4                       77.94      73.20  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(h5_path, 'data')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attributes_to_xml(obj, xml_element):\n",
    "    \"\"\" Helper function to add attributes of a group/dataset to XML element \"\"\"\n",
    "    for key, value in obj.attrs.items():\n",
    "        attr_elem = ET.SubElement(xml_element, 'Attribute', name=key)\n",
    "        attr_elem.text = str(value)\n",
    "\n",
    "def explore_h5_file_and_generate_xml(file_path, xml_output_path):\n",
    "    \"\"\" Function to explore HDF5 file and generate an XML file with metadata \"\"\"\n",
    "    # Create the root element of the XML\n",
    "    root = ET.Element('HDF5_Metadata')\n",
    "\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Function to traverse groups and datasets\n",
    "        def print_structure(name, obj):\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                # Create XML element for group\n",
    "                group_elem = ET.SubElement(root, 'Group', name=name)\n",
    "                add_attributes_to_xml(obj, group_elem)\n",
    "            elif isinstance(obj, h5py.Dataset):\n",
    "                # Create XML element for dataset\n",
    "                dataset_elem = ET.SubElement(root, 'Dataset', name=name)\n",
    "                # Add shape and dtype as attributes\n",
    "                shape_elem = ET.SubElement(dataset_elem, 'Shape')\n",
    "                shape_elem.text = str(obj.shape)\n",
    "                dtype_elem = ET.SubElement(dataset_elem, 'DataType')\n",
    "                dtype_elem.text = str(obj.dtype)\n",
    "                # Add any other attributes\n",
    "                add_attributes_to_xml(obj, dataset_elem)\n",
    "\n",
    "        # Walk through the HDF5 file structure\n",
    "        f.visititems(print_structure)\n",
    "\n",
    "    # Write the XML structure to an XML file\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(xml_output_path, encoding='utf-8', xml_declaration=True)\n",
    "    print(f\"XML metadata saved to {xml_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML metadata saved to metadata_output.xml\n"
     ]
    }
   ],
   "source": [
    "xml_output_file = 'metadata_output.xml'\n",
    "\n",
    "# Call the function to explore the HDF5 file and generate the XML file\n",
    "explore_h5_file_and_generate_xml(h5_path, xml_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
